{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lack of an explicit momentum source could cause the biases we are seeing in the SAM simulation. In this notebook, I train a neural network for the momentum source in the same manners as I did for QT and SLI. I also want to see the parametrized source that this gives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, Functions and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import pipe\n",
    "import uwnet.interface\n",
    "from uwnet.model import MLP\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from uwnet.datasets import XRTimeSeries\n",
    "from uwnet.utils import concat_dicts\n",
    "\n",
    "\n",
    "# define paths for data and nn model\n",
    "train_data_path = \"../data/processed/2018-10-02-ngaqua-subset.nc\"\n",
    "model_path = \"../models/4/9.pkl\"\n",
    "\n",
    "# load the model and training data\n",
    "data = xr.open_dataset(train_data_path)\n",
    "# mlp = MLP.from_path(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization imports\n",
    "\n",
    "from ipywidgets import interact, FloatSlider\n",
    "from gnl.colorblind import colorblind_matplotlib\n",
    "colorblind_matplotlib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_xr(model, ds, **kwargs):\n",
    "    \"\"\"Run column simulation with prescribed forcings\"\"\"\n",
    "    ds = ds.isel(z=model.z)\n",
    "    data = XRTimeSeries(ds.load(), [['time'], ['x', 'y'], ['z']])\n",
    "    loader = DataLoader(data, batch_size=1024, shuffle=False)\n",
    "\n",
    "    constants = data.torch_constants()\n",
    "\n",
    "\n",
    "    print(\"Running model\")\n",
    "    model.add_forcing = True\n",
    "    # prepare input for mod\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch.update(constants)\n",
    "            out = model(batch, **kwargs)\n",
    "            outputs.append(out)\n",
    "\n",
    "    # concatenate outputs\n",
    "    out = concat_dicts(outputs, dim=0)\n",
    "\n",
    "    def unstack(val):\n",
    "        val = val.detach().numpy()\n",
    "        dims = ['xbatch', 'xtime', 'xfeat'][:val.ndim]\n",
    "        coords = {key: data._ds.coords[key] for key in dims}\n",
    "\n",
    "        if val.shape[-1] == 1:\n",
    "            dims.pop()\n",
    "            coords.pop('xfeat')\n",
    "            val = val[..., 0]\n",
    "        ds = xr.DataArray(val, dims=dims, coords=coords)\n",
    "        for dim in dims:\n",
    "            ds = ds.unstack(dim)\n",
    "\n",
    "        # transpose dims\n",
    "        dim_order = [dim for dim in ['time', 'z', 'y', 'x'] if dim in ds.dims]\n",
    "        ds = ds.transpose(*dim_order)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    print(\"Reshaping and saving outputs\")\n",
    "    out_da = {key: unstack(val) for key, val in out.items()}\n",
    "\n",
    "    truth_vars = set(out) & set(data.data)\n",
    "    rename_dict = {key: key + 'OBS' for key in truth_vars}\n",
    "\n",
    "    ds = xr.Dataset(out_da).merge(data.data.rename(rename_dict))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_and_predict(model_path, data, **kw):\n",
    "    mlp = MLP.from_path(model_path)\n",
    "    return forward_xr(mlp, data, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis = load_and_predict(model_path, data)\n",
    "prediction = load_and_predict(model_path, data, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.U.isel(x=0).plot(x='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.UOBS.isel(x=0).plot(x='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dissipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = ['x', 'time']\n",
    "dissip_x = (prediction.FUNN * prediction.U).mean(dims)/(prediction.U**2).mean(dims)\n",
    "\n",
    "\n",
    "plt.plot(dissip_x.values*86400)\n",
    "plt.grid()\n",
    "plt.xlabel('Vertical grid number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is mostly damping in the in the free troposphere, but it is amplifying in the lowest few grid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(1/np.abs(dissip_x)/86400)\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "\n",
    "ticks = np.arange(0, dissip_x.shape[0], 5)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(dissip_x.z[ticks].values)\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Damping/growth time-scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time scales vary from around 1 day in the boundary layer to around 20 in the free troposphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift in Mean state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_drift(prediction):\n",
    "\n",
    "    fu_mean = prediction.FU.mean(['x', 'time','y'])\n",
    "    funn_mean = prediction.FUNN.mean(['x', 'time','y'])\n",
    "    du_obs = (prediction.UOBS[-1]  - prediction.UOBS[0])/(prediction.time[-1]-prediction.time[0])/86400\n",
    "    du_obs = du_obs.mean(['x', 'y'])\n",
    "\n",
    "    plt.figure(figsize=(3,6))\n",
    "\n",
    "    fu_mean.plot(y='z', label='FU')\n",
    "    funn_mean.plot(y='z', label='FUNN')\n",
    "    (funn_mean+fu_mean).plot(label='FU-FUNN', y='z')\n",
    "    (du_obs).plot(label=r'$\\Delta U / \\Delta t$', y='z')\n",
    "\n",
    "    a = 2e-5\n",
    "    plt.xlim([-a, a])\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_drift(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this problem also in the diagnosis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_drift(diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is. Perhaps we should penalize the difference in the mean drifts? Would this work batchwise? Perhaps we should also make sure the output of the neural network is reasonably smooth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
