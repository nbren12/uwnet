{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviewer asked to assess the robustness of our LRFs using a bootstrapping procedure. Here is their comment:\n",
    "\n",
    "> avoiding causality statements without further physical analysis and (2) assessing the robustness of the upper-tropospheric features by bootstrapping the linear response functions. For instance, one could calculate the level-by-level variance of the linear response function about diÙè∞Éerent states that have similar thermodynamic properties (e.g. latitude by latitude).\n",
    "\n",
    "Let's focus on the tropics, since that is where the instability is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jacobian import *\n",
    "from src.data import open_data\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(path=\"../../models/265/5.pkl\"):\n",
    "    # open model\n",
    "    model = torch.load(path)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "from random import randint\n",
    "\n",
    "def bootstrap_samples(tropics, n):\n",
    "    sample_dims = ['time', 'x', 'y']\n",
    "    dim_name = 'sample'\n",
    "    indexers = {dim: xr.DataArray(np.random.choice(tropics[dim], n), dims=[dim_name])\n",
    "                for dim in sample_dims}\n",
    "    samples_dataset = tropics.sel(**indexers)\n",
    "    \n",
    "    for i in range(n):\n",
    "        rand_ind = randint(0, n-1)\n",
    "        sample = (samples_dataset\n",
    "                  .isel(sample=rand_ind)\n",
    "                  .expand_dims(['y', 'x'], [-2, -1])\n",
    "                  .compute())\n",
    "        yield sample\n",
    "        \n",
    "        \n",
    "def get_jacobian(model, sample):\n",
    "    necessary_variables = sample[model.input_names]\n",
    "    jac = saliency_map_one_location(model, necessary_variables)\n",
    "    return jac\n",
    "\n",
    "\n",
    "# compute boot strap stats\n",
    "from collections import defaultdict\n",
    "\n",
    "def apply_list_jacobian(func, seq):\n",
    "    keys = seq[0].keys()\n",
    "    output = defaultdict(dict)\n",
    "    for ink in keys:\n",
    "        for outk in keys:\n",
    "            output[outk][ink] = func([it[outk][ink] for it in seq])\n",
    "    return output\n",
    "\n",
    "\n",
    "def mean(seq):\n",
    "    n = len(seq)\n",
    "    return sum(seq)/n\n",
    "\n",
    "def std(seq):\n",
    "    n = len(seq)\n",
    "    mu = mean(seq)\n",
    "    variance = sum((it - mu)**2 for it in seq)/n\n",
    "    return torch.sqrt(variance)\n",
    "\n",
    "\n",
    "def std_error(seq):\n",
    "    n = len(seq)\n",
    "    return std(seq)/torch.sqrt(torch.tensor(n).float())\n",
    "\n",
    "from toolz import curry\n",
    "\n",
    "@curry\n",
    "def quantile(seq, q):\n",
    "    arr = torch.stack(seq).detach().numpy()\n",
    "    ans = np.quantile(arr, q, axis=0)\n",
    "    return torch.tensor(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap sample size\n",
    "n = 100\n",
    "\n",
    "model = get_model(path=\"../../models/265/5.pkl\")\n",
    "training = open_data('training')\n",
    "training['region'] = common.get_regions(training.y)\n",
    "tropics = training.isel(y=slice(30,34))\n",
    "samples = bootstrap_samples(tropics, n)\n",
    "jacobians = [get_jacobian(model, sample) for sample in samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute some statistics accross the boot strapped samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_jacobian = apply_list_jacobian(mean, jacobians)\n",
    "std_jacobian = apply_list_jacobian(std, jacobians)\n",
    "se_jacobian = apply_list_jacobian(std_error, jacobians)\n",
    "q25_jacobian = apply_list_jacobian(quantile(q=.025), jacobians)\n",
    "q975_jacobian = apply_list_jacobian(quantile(q=.975), jacobians)\n",
    "p = training.p.values[0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot((se_jacobian, p));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure the standard error is the right thing to plot here. The question is what the significance threshold is for each pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think using the quantiles of the boot strap cycle is more relevant. Here is the 97.5-percentile of the LRF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot((q975_jacobian, p));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the 2.5% percentile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot((q25_jacobian, p));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sign reverse between these plots. I assume this is due to diurnal variability. Perhaps, I should condition these LRFS based on the time of day, or only consider the LRF averaged over the diurnal cycle. Nonetheless, the magnitude of the upper level sensitivity remains equivalent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
